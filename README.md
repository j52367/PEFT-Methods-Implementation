# PEFT Methods Implementation ğŸš€

## ğŸ“– Introduction
This repository contains implementations and experiments of various **Parameter-Efficient Fine-Tuning (PEFT)** methods using Hugging Face libraries.
The goal is to understand how to adapt large pre-trained models (LLMs) to downstream tasks with minimal computational resources.

## ğŸ¯ Implemented Methods
| Method | Description | Status |
| :--- | :--- | :---: |
| **LoRA** (Low-Rank Adaptation) | Freezes pre-trained weights and injects trainable rank decomposition matrices. | â–³ |
| **Prefix Tuning** | Optimizes a sequence of continuous task-specific vectors (prefixes). | â–³ |
| **P-Tuning** | Uses trainable prompt embeddings. | X |
| **Prompt-Tuning** |  | X |


## ğŸ› ï¸ Tech Stack
* **Python** 3.13.5
* **PyTorch**
* **Hugging Face Transformers**
* **Hugging Face PEFT**

## ğŸ“Š Experiments
(ì‹¤í—˜ ê²°ê³¼ ì¶”ê°€ ì˜ˆì •)

## ğŸ”— References
* [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
* [Hugging Face PEFT Documentation](https://huggingface.co/docs/peft/index)