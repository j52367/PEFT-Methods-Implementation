# PEFT Methods Implementation ğŸš€

## ğŸ“– Introduction
This repository contains implementations and experiments of various **Parameter-Efficient Fine-Tuning (PEFT)** methods using Hugging Face libraries.
The goal is to understand how to adapt large pre-trained models (LLMs) to downstream tasks with minimal computational resources.

## ğŸ¯ Implemented Methods
| Method | Description | Status |
| :--- | :--- | :---: |
| **LoRA** (Low-Rank Adaptation) | Freezes pre-trained weights and injects trainable rank decomposition matrices. | âœ… |
| **Prefix Tuning** | Optimizes a sequence of continuous task-specific vectors (prefixes). | ğŸš§ |
| **P-Tuning** | Uses trainable prompt embeddings. | ğŸ“… |
| **IA3** | Infused Adapter by Inhibiting and Amplifying Inner Activations. | ğŸ“… |

## ğŸ› ï¸ Tech Stack
* **Python** 3.8+
* **PyTorch**
* **Hugging Face Transformers**
* **Hugging Face PEFT**

## ğŸ“Š Experiments
(ì‹¤í—˜ ê²°ê³¼ ê·¸ë˜í”„ë‚˜ í‘œë¥¼ ì—¬ê¸°ì— ì¶”ê°€í•  ì˜ˆì •)

## ğŸ”— References
* [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
* [Hugging Face PEFT Documentation](https://huggingface.co/docs/peft/index)